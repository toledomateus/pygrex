{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recoexplainer.config import cfg\n",
    "from recoexplainer.data_reader.data_reader import DataReader\n",
    "from recoexplainer.models.gmf_model import GMFModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataReader(cfg.data)\n",
    "data.make_consecutive_ids_in_dataset()\n",
    "df = data.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gmf = GMFModel(cfg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of userId is [0, 942]\n",
      "Range of itemId is [0, 1681]\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.7395685315132141\n",
      "[Training Epoch 0] Batch 200, Loss 0.6264557838439941\n",
      "[Training Epoch 0] Batch 400, Loss 0.6617759466171265\n",
      "[Training Epoch 0] Batch 600, Loss 0.7048221826553345\n",
      "[Training Epoch 0] Batch 800, Loss 0.6100355982780457\n",
      "[Training Epoch 0] Batch 1000, Loss 0.6726314425468445\n",
      "[Training Epoch 0] Batch 1200, Loss 0.6228196620941162\n",
      "[Training Epoch 0] Batch 1400, Loss 0.7185613512992859\n",
      "[Training Epoch 0] Batch 1600, Loss 0.6002751588821411\n",
      "[Training Epoch 0] Batch 1800, Loss 0.6169261336326599\n",
      "[Training Epoch 0] Batch 2000, Loss 0.6362009048461914\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.5994158983230591\n",
      "[Training Epoch 1] Batch 200, Loss 0.6924072504043579\n",
      "[Training Epoch 1] Batch 400, Loss 0.6918842196464539\n",
      "[Training Epoch 1] Batch 600, Loss 0.7037022709846497\n",
      "[Training Epoch 1] Batch 800, Loss 0.6541235446929932\n",
      "[Training Epoch 1] Batch 1000, Loss 0.5599548816680908\n",
      "[Training Epoch 1] Batch 1200, Loss 0.5888311862945557\n",
      "[Training Epoch 1] Batch 1400, Loss 0.7773635983467102\n",
      "[Training Epoch 1] Batch 1600, Loss 0.5991183519363403\n",
      "[Training Epoch 1] Batch 1800, Loss 0.6120011210441589\n",
      "[Training Epoch 1] Batch 2000, Loss 0.625365138053894\n",
      "Epoch 2 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 2] Batch 0, Loss 0.6848688125610352\n",
      "[Training Epoch 2] Batch 200, Loss 0.5757110118865967\n",
      "[Training Epoch 2] Batch 400, Loss 0.6642931699752808\n",
      "[Training Epoch 2] Batch 600, Loss 0.6005003452301025\n",
      "[Training Epoch 2] Batch 800, Loss 0.617630124092102\n",
      "[Training Epoch 2] Batch 1000, Loss 0.6103206276893616\n",
      "[Training Epoch 2] Batch 1200, Loss 0.6233221292495728\n",
      "[Training Epoch 2] Batch 1400, Loss 0.7011667490005493\n",
      "[Training Epoch 2] Batch 1600, Loss 0.6174147725105286\n",
      "[Training Epoch 2] Batch 1800, Loss 0.7063840627670288\n",
      "[Training Epoch 2] Batch 2000, Loss 0.6180096864700317\n",
      "Epoch 3 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 3] Batch 0, Loss 0.607726514339447\n",
      "[Training Epoch 3] Batch 200, Loss 0.7033982276916504\n",
      "[Training Epoch 3] Batch 400, Loss 0.6274887323379517\n",
      "[Training Epoch 3] Batch 600, Loss 0.619849681854248\n",
      "[Training Epoch 3] Batch 800, Loss 0.6790701150894165\n",
      "[Training Epoch 3] Batch 1000, Loss 0.6802634596824646\n",
      "[Training Epoch 3] Batch 1200, Loss 0.6595026254653931\n",
      "[Training Epoch 3] Batch 1400, Loss 0.6559931635856628\n",
      "[Training Epoch 3] Batch 1600, Loss 0.7569234371185303\n",
      "[Training Epoch 3] Batch 1800, Loss 0.6353310346603394\n",
      "[Training Epoch 3] Batch 2000, Loss 0.6391507983207703\n",
      "Epoch 4 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 4] Batch 0, Loss 0.6680541038513184\n",
      "[Training Epoch 4] Batch 200, Loss 0.6138972640037537\n",
      "[Training Epoch 4] Batch 400, Loss 0.6315671801567078\n",
      "[Training Epoch 4] Batch 600, Loss 0.5934430956840515\n",
      "[Training Epoch 4] Batch 800, Loss 0.6396984457969666\n",
      "[Training Epoch 4] Batch 1000, Loss 0.5890204310417175\n",
      "[Training Epoch 4] Batch 1200, Loss 0.6221398115158081\n",
      "[Training Epoch 4] Batch 1400, Loss 0.6180979013442993\n",
      "[Training Epoch 4] Batch 1600, Loss 0.5952133536338806\n",
      "[Training Epoch 4] Batch 1800, Loss 0.5887220501899719\n",
      "[Training Epoch 4] Batch 2000, Loss 0.6485490202903748\n",
      "Epoch 5 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 5] Batch 0, Loss 0.6095282435417175\n",
      "[Training Epoch 5] Batch 200, Loss 0.6116141080856323\n",
      "[Training Epoch 5] Batch 400, Loss 0.7112008929252625\n",
      "[Training Epoch 5] Batch 600, Loss 0.6189417839050293\n",
      "[Training Epoch 5] Batch 800, Loss 0.575363039970398\n",
      "[Training Epoch 5] Batch 1000, Loss 0.63219153881073\n",
      "[Training Epoch 5] Batch 1200, Loss 0.6201850771903992\n",
      "[Training Epoch 5] Batch 1400, Loss 0.5538255572319031\n",
      "[Training Epoch 5] Batch 1600, Loss 0.6610196828842163\n",
      "[Training Epoch 5] Batch 1800, Loss 0.7013993859291077\n",
      "[Training Epoch 5] Batch 2000, Loss 0.6502764225006104\n",
      "Epoch 6 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 6] Batch 0, Loss 0.6313559412956238\n",
      "[Training Epoch 6] Batch 200, Loss 0.6742907166481018\n",
      "[Training Epoch 6] Batch 400, Loss 0.6339247822761536\n",
      "[Training Epoch 6] Batch 600, Loss 0.6568845510482788\n",
      "[Training Epoch 6] Batch 800, Loss 0.6829248666763306\n",
      "[Training Epoch 6] Batch 1000, Loss 0.6018242835998535\n",
      "[Training Epoch 6] Batch 1200, Loss 0.6687591075897217\n",
      "[Training Epoch 6] Batch 1400, Loss 0.7085936665534973\n",
      "[Training Epoch 6] Batch 1600, Loss 0.6995847225189209\n",
      "[Training Epoch 6] Batch 1800, Loss 0.6081010699272156\n",
      "[Training Epoch 6] Batch 2000, Loss 0.6254996061325073\n",
      "Epoch 7 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 7] Batch 0, Loss 0.6152509450912476\n",
      "[Training Epoch 7] Batch 200, Loss 0.6626979112625122\n",
      "[Training Epoch 7] Batch 400, Loss 0.6219613552093506\n",
      "[Training Epoch 7] Batch 600, Loss 0.6415508389472961\n",
      "[Training Epoch 7] Batch 800, Loss 0.70012366771698\n",
      "[Training Epoch 7] Batch 1000, Loss 0.6628687977790833\n",
      "[Training Epoch 7] Batch 1200, Loss 0.6481966376304626\n",
      "[Training Epoch 7] Batch 1400, Loss 0.684747576713562\n",
      "[Training Epoch 7] Batch 1600, Loss 0.6657401323318481\n",
      "[Training Epoch 7] Batch 1800, Loss 0.5909736752510071\n",
      "[Training Epoch 7] Batch 2000, Loss 0.6852108240127563\n",
      "Epoch 8 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 8] Batch 0, Loss 0.656915545463562\n",
      "[Training Epoch 8] Batch 200, Loss 0.6208204627037048\n",
      "[Training Epoch 8] Batch 400, Loss 0.6075006723403931\n",
      "[Training Epoch 8] Batch 600, Loss 0.5431362986564636\n",
      "[Training Epoch 8] Batch 800, Loss 0.649527907371521\n",
      "[Training Epoch 8] Batch 1000, Loss 0.5911492109298706\n",
      "[Training Epoch 8] Batch 1200, Loss 0.628207266330719\n",
      "[Training Epoch 8] Batch 1400, Loss 0.6299926042556763\n",
      "[Training Epoch 8] Batch 1600, Loss 0.6453256011009216\n",
      "[Training Epoch 8] Batch 1800, Loss 0.7003358602523804\n",
      "[Training Epoch 8] Batch 2000, Loss 0.6716583371162415\n",
      "Epoch 9 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 9] Batch 0, Loss 0.6904311180114746\n",
      "[Training Epoch 9] Batch 200, Loss 0.6817777752876282\n",
      "[Training Epoch 9] Batch 400, Loss 0.6396579742431641\n",
      "[Training Epoch 9] Batch 600, Loss 0.6852333545684814\n",
      "[Training Epoch 9] Batch 800, Loss 0.6001243591308594\n",
      "[Training Epoch 9] Batch 1000, Loss 0.6617239713668823\n",
      "[Training Epoch 9] Batch 1200, Loss 0.6454816460609436\n",
      "[Training Epoch 9] Batch 1400, Loss 0.6660251617431641\n",
      "[Training Epoch 9] Batch 1600, Loss 0.566120982170105\n",
      "[Training Epoch 9] Batch 1800, Loss 0.660525918006897\n",
      "[Training Epoch 9] Batch 2000, Loss 0.602811872959137\n"
     ]
    }
   ],
   "source": [
    "gmf.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
